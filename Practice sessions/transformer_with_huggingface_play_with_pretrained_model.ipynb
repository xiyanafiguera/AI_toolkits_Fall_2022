{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install hugging face transformer.\n",
        "[MORE DETAILED CODES ARE HERE](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb#scrollTo=rUsYoEj3kM4N)"
      ],
      "metadata": {
        "id": "x4VGNsvd0pdn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krhjU_nqyKCV"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "Y-vmiBSB12L9",
        "outputId": "d5040248-a1e1-427b-b129-98d11c8aab73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/display.py:701: UserWarning: Consider using IPython.display.IFrame instead\n",
            "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Source](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb#scrollTo=2iCHdTn7kM4I)\n",
        "\n",
        "\n",
        "- The [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) is the easiest way to use a pretrained model for inference. You can use the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) out-of-the-box for many tasks across different modalities. Take a look at the table below for some supported tasks:\n",
        "\n",
        "| **Task**                     | **Description**                                                                                              | **Modality**    | **Pipeline identifier**                       |\n",
        "|------------------------------|--------------------------------------------------------------------------------------------------------------|-----------------|-----------------------------------------------|\n",
        "| Text classification          | assign a label to a given sequence of text                                                                   | NLP             | pipeline(task=\"sentiment-analysis\")           |\n",
        "| Text generation              | generate text that follows a given prompt                                                                    | NLP             | pipeline(task=\"text-generation\")              |\n",
        "| Name entity recognition      | assign a label to each token in a sequence (people, organization, location, etc.)                            | NLP             | pipeline(task=\"ner\")                          |\n",
        "| Question answering           | extract an answer from the text given some context and a question                                            | NLP             | pipeline(task=\"question-answering\")           |\n",
        "| Fill-mask                    | predict the correct masked token in a sequence                                                               | NLP             | pipeline(task=\"fill-mask\")                    |\n",
        "| Summarization                | generate a summary of a sequence of text or document                                                         | NLP             | pipeline(task=\"summarization\")                |\n",
        "| Translation                  | translate text from one language into another                                                                | NLP             | pipeline(task=\"translation\")                  |\n",
        "| Image classification         | assign a label to an image                                                                                   | Computer vision | pipeline(task=\"image-classification\")         |\n",
        "| Image segmentation           | assign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation) | Computer vision | pipeline(task=\"image-segmentation\")           |\n",
        "| Object detection             | predict the bounding boxes and classes of objects in an image                                                | Computer vision | pipeline(task=\"object-detection\")             |\n",
        "| Audio classification         | assign a label to an audio file                                                                              | Audio           | pipeline(task=\"audio-classification\")         |\n",
        "| Automatic speech recognition | extract speech from an audio file into text                                                                  | Audio           | pipeline(task=\"automatic-speech-recognition\") |\n",
        "| Visual question answering    | given an image and a question, correctly answer a question about the image                                   | Multimodal      | pipeline(task=\"vqa\")                          |\n",
        "\n",
        "Start by creating an instance of [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) and specifying a task you want to use it for. You can use the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) for any of the previously mentioned tasks, and for a complete list of supported tasks, take a look at the [pipeline API reference](https://huggingface.co/docs/transformers/main/en/./main_classes/pipelines)."
      ],
      "metadata": {
        "id": "ioARbQ6anbnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define various pretrained model for various tasks"
      ],
      "metadata": {
        "id": "GimS5n-F1Hg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(task=\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "generator = pipeline(task='text-generation', model=\"distilgpt2\") \n",
        "fill_mask = pipeline(task='fill-mask', model='bert-base-uncased')\n",
        "question_answering = pipeline(task='question-answering', model=\"deepset/roberta-base-squad2\", tokenizer='deepset/roberta-base-squad2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PsErVbfkkdp",
        "outputId": "4cb01fa1-a2a7-466d-9556-264ac59d429b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Classifier"
      ],
      "metadata": {
        "id": "B4uOVo6r1MjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\"It is too cold outside. I don't want to go outside today.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEH9RWIzk8dk",
        "outputId": "ea160bd0-d77b-4987-94cc-1533e4a25af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'negative', 'score': 0.906129777431488}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = classifier([\"It is too cold outside. I don't want to go outside today.\", \"But Christmas is coming!\"])\n",
        "for res in results:\n",
        "    print(\"label: {}, with score: {}\".format(res['label'], res['score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyFNl7-klCRt",
        "outputId": "23227ce6-0162-4d76-88bc-61cb84fafd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label: NEGATIVE, with score: 0.9985353946685791\n",
            "label: POSITIVE, with score: 0.999188244342804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generator"
      ],
      "metadata": {
        "id": "LZTUIjRm1SPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = generator(['Baby it is cold outside', 'Finally it is summer time'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3WamGB0lM24",
        "outputId": "0e0bcf96-f0e6-468f-a3b1-42eeedaf0b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'generated_text': \"Baby it is cold outside—and when I take it back the next day I will have my cold sweats again. It will feel like I am on a cold side.\\n\\nI'm not as stupid as anyone thinks I am. It's something\"}], [{'generated_text': 'Finally it is summer time at school and with the new season on the horizon we have the opportunity to start taking a look at all the new signings that have stepped up our coaching staff,\" said coach Dave Aranda.'}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results[0][0]['generated_text'])\n",
        "print('*'*50)\n",
        "print(results[1][0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eAzAbt8oEU3",
        "outputId": "5bb4884d-76e8-4f43-e6b9-bed49e08f76f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baby it is cold outside—and when I take it back the next day I will have my cold sweats again. It will feel like I am on a cold side.\n",
            "\n",
            "I'm not as stupid as anyone thinks I am. It's something\n",
            "**************************************************\n",
            "Finally it is summer time at school and with the new season on the horizon we have the opportunity to start taking a look at all the new signings that have stepped up our coaching staff,\" said coach Dave Aranda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fill Mask"
      ],
      "metadata": {
        "id": "_z_cQFc71Tf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fill_results = fill_mask('A man is in a kitchen, and he is holding an empty mug. He walks towards a coffee machine to get [MASK].')"
      ],
      "metadata": {
        "id": "MXikLm4mq45x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fill_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbOhCL1QrKhG",
        "outputId": "444f9806-098c-4d31-d40a-9182b920a7a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.2893911898136139, 'token': 4157, 'token_str': 'coffee', 'sequence': 'a man is in a kitchen, and he is holding an empty mug. he walks towards a coffee machine to get coffee.'}, {'score': 0.2196974754333496, 'token': 2009, 'token_str': 'it', 'sequence': 'a man is in a kitchen, and he is holding an empty mug. he walks towards a coffee machine to get it.'}, {'score': 0.1663384586572647, 'token': 2028, 'token_str': 'one', 'sequence': 'a man is in a kitchen, and he is holding an empty mug. he walks towards a coffee machine to get one.'}, {'score': 0.09720255434513092, 'token': 2242, 'token_str': 'something', 'sequence': 'a man is in a kitchen, and he is holding an empty mug. he walks towards a coffee machine to get something.'}, {'score': 0.03316616266965866, 'token': 2300, 'token_str': 'water', 'sequence': 'a man is in a kitchen, and he is holding an empty mug. he walks towards a coffee machine to get water.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question and Answering"
      ],
      "metadata": {
        "id": "LB2KqvdV1V_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QA_input = {'question': 'What should a robot bring to man?', \n",
        "            'context': fill_results[0]['sequence']}\n",
        "results = question_answering(QA_input)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOndbzXYrd5x",
        "outputId": "670d3ad7-45ab-446d-b746-2bb5a98a84fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.041885215789079666, 'start': 96, 'end': 102, 'answer': 'coffee'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Similarity based on the sentence bert"
      ],
      "metadata": {
        "id": "R8vOWX8d1bqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\""
      ],
      "metadata": {
        "id": "32rXlbT2s5Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "feature_extractor = pipeline('feature-extraction', model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "Z4vcpl3dwtVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The first method - non parallel computation"
      ],
      "metadata": {
        "id": "3oEKMFKx1f4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['In this paper, we propose a novel method called Transformer.', 'We propose a novel model named Transformer.']\n",
        "\n",
        "feature_1 = torch.Tensor(feature_extractor(sentences[0]))\n",
        "feature_2 = torch.Tensor(feature_extractor(sentences[1]))\n",
        "\n",
        "norm_feat_1 = F.normalize(torch.mean(feature_1, dim=1), dim=-1)\n",
        "norm_feat_2 = F.normalize(torch.mean(feature_2, dim=1), dim=-1)\n",
        "similarity = torch.sum( norm_feat_1 * norm_feat_2 )\n",
        "print('TWO SENTENCE Similarity: ', similarity.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNmR1aUVw8at",
        "outputId": "d679f848-ffad-47a5-e7c7-f166ed050029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TWO SENTENCE Similarity:  0.7857804894447327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The sceond method - parallel computation with padding (recommended)"
      ],
      "metadata": {
        "id": "nv0i0_bX1p8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "print(encoded_input)\n",
        "print(encoded_input['attention_mask'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwXhOOZCxGL9",
        "outputId": "ed05e424-313f-4fbc-a772-6f0e44a18dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1999,  2023,  3259,  1010,  2057, 16599,  1037,  3117,  4118,\n",
            "          2170, 10938,  2121,  1012,   102],\n",
            "        [  101,  2057, 16599,  1037,  3117,  2944,  2315, 10938,  2121,  1012,\n",
            "           102,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n",
            "torch.Size([2, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "print(model_output.keys())\n",
        "print(model_output['last_hidden_state'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvUjqHLYxHbp",
        "outputId": "5d6936c2-5fa9-4409-cd35-13ee4042e8a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['last_hidden_state', 'pooler_output'])\n",
            "torch.Size([2, 15, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
      ],
      "metadata": {
        "id": "iWnt4Xx9xKpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)"
      ],
      "metadata": {
        "id": "U7ISbeDSxh2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = (sentence_embeddings[0] * sentence_embeddings[1]).sum()\n",
        "print('TWO SENTENCE SIMILARITY: ', similarity.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcHn3pvmyVJS",
        "outputId": "9d8c53f0-4799-41c5-bd89-16ce72affb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TWO SENTENCE SIMILARITY:  0.7857804298400879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QX0vtQSIygjD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}